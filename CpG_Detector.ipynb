{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4T6QHHOnfcQ"
   },
   "source": [
    "# Part 1: Build CpG Detector\n",
    "\n",
    "Here we have a simple problem, given a DNA sequence (of N, A, C, G, T), count the number of CpGs in the sequence (consecutive CGs).\n",
    "\n",
    "We have defined a few helper functions / parameters for performing this task.\n",
    "\n",
    "We need you to build a LSTM model and train it to complish this task in PyTorch.\n",
    "\n",
    "A good solution will be a model that can be trained, with high confidence in correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mfS4cLmZD2oB"
   },
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from functools import partial\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_f-brPAvKvTn"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE HERE\n",
    "def set_seed(seed=13):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(13)\n",
    "\n",
    "# Use this for getting x label\n",
    "def rand_sequence(n_seqs: int, seq_len: int=128) -> Sequence[int]:\n",
    "    for i in range(n_seqs):\n",
    "        yield [random.randint(0, 4) for _ in range(seq_len)]\n",
    "\n",
    "# Use this for getting y label\n",
    "def count_cpgs(seq: str) -> int:\n",
    "    cgs = 0\n",
    "    for i in range(0, len(seq) - 1):\n",
    "        dimer = seq[i:i+2]\n",
    "        # note that seq is a string, not a list\n",
    "        if dimer == \"CG\":\n",
    "            cgs += 1\n",
    "    return cgs\n",
    "\n",
    "# Alphabet helpers   \n",
    "alphabet = 'NACGT'\n",
    "dna2int = { a: i for a, i in zip(alphabet, range(5))}\n",
    "int2dna = { i: a for a, i in zip(alphabet, range(5))}\n",
    "\n",
    "intseq_to_dnaseq = partial(map, int2dna.get)\n",
    "dnaseq_to_intseq = partial(map, dna2int.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 383,
     "status": "ok",
     "timestamp": 1651686469847,
     "user": {
      "displayName": "Ylex",
      "userId": "01820639168093643789"
     },
     "user_tz": 240
    },
    "id": "VK9Qg5GHYxOb",
    "outputId": "0a00bbb6-d9ac-4cf8-ed84-b55b335d7f51"
   },
   "outputs": [],
   "source": [
    "def prepare_data(num_samples=100):\n",
    "    sequences = list(rand_sequence(num_samples))\n",
    "    temp = [\"\".join(list(intseq_to_dnaseq(seq))) for seq in sequences]\n",
    "    labels = [count_cpgs(seq) for seq in temp]\n",
    "    return sequences, labels\n",
    "    \n",
    "train_x, train_y = prepare_data(2048)\n",
    "test_x, test_y = prepare_data(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "VOCAB_SIZE = 5\n",
    "LSTM_HIDDEN = 32\n",
    "LSTM_LAYER = 4\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "epoch_num = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 128, 5]) torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# Data loader and shape check\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DnaDataset(Dataset):\n",
    "    def __init__(self, sequences, counts):\n",
    "        self.sequences = sequences\n",
    "        self.counts = counts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.LongTensor(self.sequences[index]), self.counts[index]\n",
    "\n",
    "\n",
    "class Collater:\n",
    "    def __call__(self, batch):\n",
    "        sequences, labels = zip(*batch)\n",
    "        sequences = torch.stack(sequences)\n",
    "        sequences = F.one_hot(sequences)\n",
    "        labels = torch.tensor(labels, dtype=torch.float)\n",
    "        return sequences.to(torch.float32), labels.to(torch.float32)\n",
    "        \n",
    "\n",
    "collate_fn = Collater()\n",
    "training_data = DnaDataset(train_x, train_y)\n",
    "testing_data = DnaDataset(test_x, test_y)\n",
    "train_data_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_data_loader = DataLoader(testing_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "for i, batch in enumerate(train_data_loader):\n",
    "    x, y = batch\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "q8fgxrM0LnLy"
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "class CpGPredictor(torch.nn.Module):\n",
    "    ''' Simple model that uses a LSTM to count the number of CpGs in a sequence '''\n",
    "    def __init__(self):\n",
    "        super(CpGPredictor, self).__init__()\n",
    "        self.lstm = nn.LSTM(VOCAB_SIZE, LSTM_HIDDEN, LSTM_LAYER, batch_first=True)\n",
    "        self.classifier = nn.Linear(LSTM_HIDDEN, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded, _ = self.lstm(x)\n",
    "        encoded = torch.sum(encoded, dim=1)\n",
    "        logits = self.classifier(encoded).squeeze(-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model / loss function / optimizer etc.\n",
    "model = CpGPredictor()\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n",
    "def train_one_epoch(model, train_data_loader, optimizer, loss_fn):\n",
    "    t_loss = .0\n",
    "    for j, batch in enumerate(train_data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x, y = batch\n",
    "        y_hat = model(x)\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        \n",
    "        t_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return t_loss / (j+1)\n",
    "\n",
    "def train(model, train_data_loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    for i, epoch in enumerate(range(epoch_num)):\n",
    "        t_loss = train_one_epoch(model, train_data_loader, optimizer, loss_fn)\n",
    "        if i % 10 == 9:\n",
    "            print(f\"{epoch+1}: {t_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10: 0.767889107693918\n",
      "20: 0.10669196597154951\n",
      "30: 0.02465420945736696\n",
      "40: 0.01395535550909699\n",
      "50: 0.0045007479457126465\n",
      "60: 0.0027540832485328792\n",
      "70: 0.002481040032080273\n",
      "80: 0.00042396134654154594\n",
      "90: 0.0003836447443461566\n",
      "100: 0.0007991211390390163\n"
     ]
    }
   ],
   "source": [
    "train(model, train_data_loader, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluation loop\n",
    "\n",
    "def eval(model, test_data_loader):\n",
    "    model.eval()\n",
    "    res_gs = []\n",
    "    res_pred = []\n",
    "    \n",
    "    for i, batch in enumerate(test_data_loader):\n",
    "        x, y = batch\n",
    "        y_hat = model(x)\n",
    "        res_pred.append(y_hat)\n",
    "        if i == 0:\n",
    "            print(y.to(torch.int32).tolist())\n",
    "            print([round(item, 2) for item in y_hat.tolist()])\n",
    "        \n",
    "        gs = torch.sum((y_hat - y) ** 2)\n",
    "        res_gs.append(gs)\n",
    "    print(round((sum(res_gs) / (batch_size * len(res_gs))).item(), 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 5, 8, 7, 4, 7, 6, 6, 3, 3, 4, 4, 1, 5, 5, 4]\n",
      "[2.03, 4.99, 8.01, 7.03, 4.01, 7.01, 6.01, 6.03, 3.0, 3.01, 4.01, 4.03, 1.02, 4.99, 5.01, 4.0]\n",
      "0.00023319\n"
     ]
    }
   ],
   "source": [
    "# TODO complete evaluation of the model\n",
    "eval(model, test_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMrRf_aVDRJm"
   },
   "source": [
    "# Part 2: what if the DNA sequences are not the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint we will need following imports\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "AKvG-MNuXJr9"
   },
   "outputs": [],
   "source": [
    "# DO NOT CHANGE HERE\n",
    "random.seed(13)\n",
    "\n",
    "# Use this for getting x label\n",
    "def rand_sequence_var_len(n_seqs: int, lb: int=16, ub: int=128) -> Sequence[int]:\n",
    "    for i in range(n_seqs):\n",
    "        seq_len = random.randint(lb, ub)\n",
    "        yield [random.randint(1, 5) for _ in range(seq_len)]\n",
    "\n",
    "\n",
    "# Use this for getting y label\n",
    "def count_cpgs(seq: str) -> int:\n",
    "    cgs = 0\n",
    "    for i in range(0, len(seq) - 1):\n",
    "        dimer = seq[i:i+2]\n",
    "        # note that seq is a string, not a list\n",
    "        if dimer == \"CG\":\n",
    "            cgs += 1\n",
    "    return cgs\n",
    "\n",
    "\n",
    "# Alphabet helpers   \n",
    "alphabet = 'NACGT'\n",
    "dna2int = {a: i for a, i in zip(alphabet, range(1, 6))}\n",
    "int2dna = {i: a for a, i in zip(alphabet, range(1, 6))}\n",
    "dna2int.update({\"<pad>\": 0})\n",
    "int2dna.update({0: \"<pad>\"})\n",
    "\n",
    "intseq_to_dnaseq = partial(map, int2dna.get)\n",
    "dnaseq_to_intseq = partial(map, dna2int.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(num_samples=100, min_len=16, max_len=128):\n",
    "    sequences = list(rand_sequence_var_len(num_samples, min_len, max_len))\n",
    "    temp = [\"\".join(list(intseq_to_dnaseq(seq))) for seq in sequences]\n",
    "    labels = [count_cpgs(seq) for seq in temp]\n",
    "    return sequences, labels\n",
    "    \n",
    "    \n",
    "min_len, max_len = 64, 128\n",
    "train_x, train_y = prepare_data(2048, min_len, max_len)\n",
    "test_x, test_y = prepare_data(512, min_len, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 125, 6]) torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nf/q8y8xg9133z4tkrjqsjm0_8m0000gn/T/ipykernel_57022/3406015071.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequences = [torch.tensor(seq) for seq in sequences]\n"
     ]
    }
   ],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sequences, counts) -> None:\n",
    "        self.sequences = sequences\n",
    "        self.counts = counts\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.LongTensor(self.sequences[index]), self.counts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    \n",
    "# this will be a collate_fn for dataloader to pad sequence  \n",
    "class PadSequence:\n",
    "    def __call__(self, batch):\n",
    "        sequences, labels = zip(*batch)\n",
    "        sequences = [torch.tensor(seq) for seq in sequences]\n",
    "        lengths = torch.tensor([len(seq) for seq in sequences])\n",
    "        padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "        padded_sequences = F.one_hot(padded_sequences)\n",
    "\n",
    "        lengths, sort_indices = lengths.sort(descending=True)\n",
    "        padded_sequences = padded_sequences[sort_indices]\n",
    "        labels = torch.tensor(labels)[sort_indices]\n",
    "\n",
    "        packed_sequences = pack_padded_sequence(padded_sequences, lengths, batch_first=True)\n",
    "        return packed_sequences.to(torch.float32), labels.to(torch.float32)\n",
    "\n",
    "collate_fn = PadSequence()\n",
    "training_data = MyDataset(train_x, train_y)\n",
    "testing_data = MyDataset(test_x, test_y)\n",
    "train_data_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_data_loader = DataLoader(testing_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "for i, batch in enumerate(train_data_loader):\n",
    "    x, y = batch\n",
    "    print(pad_packed_sequence(x, batch_first=True)[0].shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "VOCAB_SIZE = 6\n",
    "LSTM_HIDDEN = 32\n",
    "LSTM_LAYER = 4\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "epoch_num = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "class CpGPredictor(torch.nn.Module):\n",
    "    ''' Simple model that uses a LSTM to count the number of CpGs in a sequence '''\n",
    "    def __init__(self):\n",
    "        super(CpGPredictor, self).__init__()\n",
    "        self.lstm = nn.LSTM(VOCAB_SIZE, LSTM_HIDDEN, LSTM_LAYER, batch_first=True)\n",
    "        self.classifier = nn.Linear(LSTM_HIDDEN, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        packed_output, _ = self.lstm(x)\n",
    "        encoded, input_sizes = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        encoded_sum = torch.sum(encoded, dim=1)\n",
    "        logits = self.classifier(encoded_sum).squeeze(-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model / loss function / optimizer etc.\n",
    "model = CpGPredictor()\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nf/q8y8xg9133z4tkrjqsjm0_8m0000gn/T/ipykernel_57022/3406015071.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequences = [torch.tensor(seq) for seq in sequences]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10: 0.9518940644338727\n",
      "20: 0.06461269300780259\n",
      "30: 0.016101358594823978\n",
      "40: 0.0043148350664523605\n",
      "50: 0.002812138138779119\n",
      "60: 0.0012343386067641404\n",
      "70: 0.0009755911191859923\n",
      "80: 0.0015113188141526734\n",
      "90: 0.0002691288632092892\n",
      "100: 0.0005508342567850377\n"
     ]
    }
   ],
   "source": [
    "# training (you can modify the code below)\n",
    "train(model, train_data_loader, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 5, 4, 9, 6, 3, 7, 5, 5, 7, 1, 3, 3, 2, 4, 3]\n",
      "[1.01, 5.04, 4.01, 9.06, 6.04, 3.02, 7.03, 5.03, 5.03, 7.03, 1.01, 3.02, 3.02, 2.02, 4.03, 3.02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nf/q8y8xg9133z4tkrjqsjm0_8m0000gn/T/ipykernel_57022/3406015071.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequences = [torch.tensor(seq) for seq in sequences]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00051393\n"
     ]
    }
   ],
   "source": [
    "eval(model, test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'autonomize_lstm.pt')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Xi Yangs Copy of broken-nn-template.ipynb",
   "provenance": [
    {
     "file_id": "13GlbI_pdKNES8I718iwl1KNnMZ73iOOn",
     "timestamp": 1651680757732
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
